{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal FedMosaic-style toy (2 clients, hetero dims)\n",
    "- Two clients with different hidden dims (4 vs 6) and rank r=2.\n",
    "- Freeze LoRA A/B; train only PQ and gating $\\beta$.\n",
    "- Relevance via EMA + noise + subsampling of last-layer grads from a frozen small model.\n",
    "- Relevance-weighted aggregation builds per-client global PQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "def softmax_rows(x, tau=1.0):\n",
    "    z = x / tau\n",
    "    z = z - z.max(axis=1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / e.sum(axis=1, keepdims=True)\n",
    "\n",
    "def cosine(a, b):\n",
    "    a_flat, b_flat = a.ravel(), b.ravel()\n",
    "    denom = (np.linalg.norm(a_flat) * np.linalg.norm(b_flat) + 1e-9)\n",
    "    return float(np.dot(a_flat, b_flat) / denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy data and heterogeneous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = 2\n",
    "hidden_dims = [4, 6]  # hetero input/hidden sizes per client\n",
    "output_dim = 2        # keep output dim shared\n",
    "rank = 2              # low-rank size for LoRA/PQ\n",
    "\n",
    "def make_data(mean, n=64):\n",
    "    d = len(mean)\n",
    "    x = rng.normal(mean, 0.3, size=(n, d))\n",
    "    y = np.tile(mean[:output_dim], (n, 1))  # simple target depends on first two dims\n",
    "    return x, y\n",
    "\n",
    "data = [\n",
    "    make_data(np.array([1.0, 0.0, 0.5, -0.3])),\n",
    "    make_data(np.array([0.0, 1.0, -0.2, 0.4, 0.3, -0.1]))\n",
    "]\n",
    "\n",
    "# Frozen base weights and LoRA A/B (remain fixed)\n",
    "Wp = [rng.normal(0, 0.5, size=(output_dim, h)) for h in hidden_dims]\n",
    "A = [rng.normal(0, 0.3, size=(rank, h)) for h in hidden_dims]\n",
    "B = [rng.normal(0, 0.3, size=(output_dim, rank)) for _ in hidden_dims]\n",
    "\n",
    "# Trainable PQ and gating beta (one layer per client)\n",
    "P_local = [np.zeros((rank, rank)) for _ in hidden_dims]\n",
    "Q_local = [np.zeros(rank) for _ in hidden_dims]\n",
    "beta = [0.5 for _ in hidden_dims]\n",
    "\n",
    "# Server-side aggregated PQ for each client (start at zeros)\n",
    "P_global = [np.zeros_like(P_local[i]) for i in range(clients)]\n",
    "Q_global = [np.zeros_like(Q_local[i]) for i in range(clients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PQ-LoRA forward and local update\n",
    "- $h_L = B (P A h_I + Q)$ and $h_G$ uses aggregated $P, Q$.\n",
    "- $h_O = W_p h_I + (1-\\beta) h_L + \\beta h_G$.\n",
    "- Update only $P, Q, \\beta$ with MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pq_forward(x, Wp_i, A_i, B_i, P_i, Q_i):\n",
    "    # x: [batch, d_in]\n",
    "    hidden_proj = A_i @ x.T                       # [r, batch]\n",
    "    low_rank = P_i @ hidden_proj + Q_i[:, None]   # [r, batch]\n",
    "    h = (B_i @ low_rank).T                        # [batch, d_out]\n",
    "    base = (Wp_i @ x.T).T                         # [batch, d_out]\n",
    "    return base, h\n",
    "\n",
    "def local_step(idx, x, y, lr=0.3):\n",
    "    Wp_i, A_i, B_i = Wp[idx], A[idx], B[idx]\n",
    "    P_i, Q_i = P_local[idx], Q_local[idx]\n",
    "    P_g, Q_g = P_global[idx], Q_global[idx]\n",
    "    beta_i = beta[idx]\n",
    "\n",
    "    base, h_L = pq_forward(x, Wp_i, A_i, B_i, P_i, Q_i)\n",
    "    _, h_G = pq_forward(x, Wp_i, A_i, B_i, P_g, Q_g)\n",
    "    h_O = base + (1 - beta_i) * h_L + beta_i * h_G\n",
    "\n",
    "    err = h_O - y\n",
    "    bsz = len(x)\n",
    "    grad_out = (2.0 / bsz) * err                   # dL/dh_O\n",
    "\n",
    "    # Grad beta: derivative of mix between h_G and h_L\n",
    "    grad_beta = np.sum(grad_out * (h_G - h_L))\n",
    "\n",
    "    # Grad P/Q via chain rule through low-rank path\n",
    "    err_proj = grad_out @ B_i                      # [batch, r]\n",
    "    hidden_proj = (A_i @ x.T).T                    # [batch, r]\n",
    "    grad_P = err_proj.T @ hidden_proj              # [r, r]\n",
    "    grad_Q = err_proj.sum(axis=0)                  # [r]\n",
    "\n",
    "    # Parameter updates (A/B frozen)\n",
    "    P_local[idx] = P_i - lr * grad_P\n",
    "    Q_local[idx] = Q_i - lr * grad_Q\n",
    "    beta[idx] = beta_i - lr * grad_beta\n",
    "\n",
    "def batch_from_client(idx, batch_size=8):\n",
    "    x, y = data[idx]\n",
    "    sel = rng.choice(len(x), size=batch_size, replace=False)\n",
    "    return x[sel], y[sel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance via EMA + noise + subsampling\n",
    "Use a tiny frozen model $W_s$ (2Ã—2) on the first two features to compute last-layer gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_s = rng.normal(0, 1.0, size=(2, 2))  # frozen small model\n",
    "g_ema = [np.zeros_like(W_s) for _ in range(clients)]\n",
    "\n",
    "def last_layer_grad(x, y):\n",
    "    x2 = x[:, :2]  # use first two dims\n",
    "    y2 = y[:, :2]\n",
    "    pred = (W_s @ x2.T).T  # [batch, 2]\n",
    "    err = pred - y2\n",
    "    bsz = len(x2)\n",
    "    grad = (2.0 / bsz) * (err.T @ x2)  # [2,2]\n",
    "    return grad\n",
    "\n",
    "def sanitize_grad(g, noise=0.05, subsample=0.6):\n",
    "    noisy = g + rng.normal(0, noise, size=g.shape)\n",
    "    flat = noisy.ravel()\n",
    "    k = max(1, int(len(flat) * subsample))\n",
    "    mask_idx = rng.choice(len(flat), size=k, replace=False)\n",
    "    mask = np.zeros_like(flat)\n",
    "    mask[mask_idx] = 1\n",
    "    return (flat * mask).reshape(g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a few federated rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Relevance S:\n",
      " [[1.    0.919]\n",
      " [0.919 1.   ]]\n",
      "Weights per client (rows):\n",
      " [[0.54 0.46]\n",
      " [0.46 0.54]]\n",
      "Beta values: [np.float64(0.423), np.float64(0.414)]\n",
      "P norms (local): [np.float64(0.185), np.float64(0.211)]\n",
      "P norms (global): [np.float64(0.164), np.float64(0.167)]\n",
      "------------------------------------------------------------\n",
      "Round 2\n",
      "Relevance S:\n",
      " [[1. 0.]\n",
      " [0. 1.]]\n",
      "Weights per client (rows):\n",
      " [[0.881 0.119]\n",
      " [0.119 0.881]]\n",
      "Beta values: [np.float64(0.235), np.float64(0.197)]\n",
      "P norms (local): [np.float64(0.332), np.float64(0.381)]\n",
      "P norms (global): [np.float64(0.314), np.float64(0.353)]\n",
      "------------------------------------------------------------\n",
      "Round 3\n",
      "Relevance S:\n",
      " [[1.    0.823]\n",
      " [0.823 1.   ]]\n",
      "Weights per client (rows):\n",
      " [[0.587 0.413]\n",
      " [0.413 0.587]]\n",
      "Beta values: [np.float64(0.178), np.float64(0.092)]\n",
      "P norms (local): [np.float64(0.425), np.float64(0.54)]\n",
      "P norms (global): [np.float64(0.393), np.float64(0.417)]\n",
      "------------------------------------------------------------\n",
      "Sample output client 0 (first row): [ 0.013 -1.026]\n",
      "Target (first row): [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "rounds = 3\n",
    "local_steps = 4\n",
    "alpha = 0.6  # EMA decay\n",
    "tau = 0.5    # softmax temperature for relevance\n",
    "\n",
    "for r in range(rounds):\n",
    "    # Client-side local training\n",
    "    for i in range(clients):\n",
    "        for _ in range(local_steps):\n",
    "            x_b, y_b = batch_from_client(i)\n",
    "            local_step(i, x_b, y_b, lr=0.1)\n",
    "\n",
    "        # Gradient for relevance from frozen small model\n",
    "        x_g, y_g = batch_from_client(i)\n",
    "        g_now = last_layer_grad(x_g, y_g)\n",
    "        g_ema[i] = (1 - alpha) * g_ema[i] + alpha * g_now\n",
    "\n",
    "    # Server-side relevance + aggregation\n",
    "    sanitized = [sanitize_grad(g) for g in g_ema]\n",
    "    S = np.zeros((clients, clients))\n",
    "    for i in range(clients):\n",
    "        for j in range(clients):\n",
    "            S[i, j] = cosine(sanitized[i], sanitized[j])\n",
    "\n",
    "    weights = softmax_rows(S, tau=tau)\n",
    "\n",
    "    # Build customized global PQ per client\n",
    "    P_new, Q_new = [], []\n",
    "    for i in range(clients):\n",
    "        P_i = sum(weights[i, j] * P_local[j] for j in range(clients))\n",
    "        Q_i = sum(weights[i, j] * Q_local[j] for j in range(clients))\n",
    "        P_new.append(P_i)\n",
    "        Q_new.append(Q_i)\n",
    "\n",
    "    P_global, Q_global = P_new, Q_new\n",
    "\n",
    "    print(f\"Round {r+1}\")\n",
    "    print(\"Relevance S:\\n\", np.round(S, 3))\n",
    "    print(\"Weights per client (rows):\\n\", np.round(weights, 3))\n",
    "    print(\"Beta values:\", [round(b, 3) for b in beta])\n",
    "    print(\"P norms (local):\", [round(np.linalg.norm(P_local[i]), 3) for i in range(clients)])\n",
    "    print(\"P norms (global):\", [round(np.linalg.norm(P_global[i]), 3) for i in range(clients)])\n",
    "    print('-' * 60)\n",
    "\n",
    "# Quick sanity: forward one batch with final globals\n",
    "x0, y0 = batch_from_client(0)\n",
    "base0, hL0 = pq_forward(x0, Wp[0], A[0], B[0], P_local[0], Q_local[0])\n",
    "_, hG0 = pq_forward(x0, Wp[0], A[0], B[0], P_global[0], Q_global[0])\n",
    "hO0 = base0 + (1 - beta[0]) * hL0 + beta[0] * hG0\n",
    "print(\"Sample output client 0 (first row):\", np.round(hO0[0], 3))\n",
    "print(\"Target (first row):\", np.round(y0[0], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printouts show the relevance matrix, softmax weights, learned $\\beta$, and the norms of local/global $P$ after each round. Adjust `rounds`, `local_steps`, or noise/subsample ratios to explore behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
