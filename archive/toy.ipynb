{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c0eda3",
   "metadata": {},
   "source": [
    "#### Step 1: Initialize Model Parameters and Gradients\n",
    "\n",
    "We'll assume three clients, each having some local model parameters and local gradients for simplicity.\n",
    "\n",
    "Client Model Parameters (simplified)\n",
    "\n",
    "For each client, the model parameters are initialized as random matrices. The gradients for the last layer will also be computed randomly based on a given batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e98639c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize model parameters for each client\n",
    "np.random.seed(42)\n",
    "clients = 3\n",
    "hidden_dim = 4  # Let's assume the hidden layer size is 4 for simplicity\n",
    "\n",
    "# Local model weights for each client\n",
    "W1 = np.random.rand(hidden_dim, hidden_dim)\n",
    "W2 = np.random.rand(hidden_dim, hidden_dim)\n",
    "W3 = np.random.rand(hidden_dim, hidden_dim)\n",
    "\n",
    "# Assume each client has gradients from their last layer\n",
    "grad1 = np.random.rand(hidden_dim, hidden_dim)\n",
    "grad2 = np.random.rand(hidden_dim, hidden_dim)\n",
    "grad3 = np.random.rand(hidden_dim, hidden_dim)\n",
    "\n",
    "# Collecting gradients from clients\n",
    "gradients = [grad1, grad2, grad3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2e73a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Gradients:\n",
      "Client 1 Gradient:\n",
      "[[0.54671028 0.18485446 0.96958463 0.77513282]\n",
      " [0.93949894 0.89482735 0.59789998 0.92187424]\n",
      " [0.0884925  0.19598286 0.04522729 0.32533033]\n",
      " [0.38867729 0.27134903 0.82873751 0.35675333]]\n",
      "\n",
      "Client 2 Gradient:\n",
      "[[0.28093451 0.54269608 0.14092422 0.80219698]\n",
      " [0.07455064 0.98688694 0.77224477 0.19871568]\n",
      " [0.00552212 0.81546143 0.70685734 0.72900717]\n",
      " [0.77127035 0.07404465 0.35846573 0.11586906]]\n",
      "\n",
      "Client 3 Gradient:\n",
      "[[0.86310343 0.62329813 0.33089802 0.06355835]\n",
      " [0.31098232 0.32518332 0.72960618 0.63755747]\n",
      " [0.88721274 0.47221493 0.11959425 0.71324479]\n",
      " [0.76078505 0.5612772  0.77096718 0.4937956 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Client Gradients:\")\n",
    "for i, grad in enumerate(gradients, 1):\n",
    "    print(f\"Client {i} Gradient:\\n{grad}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d7856",
   "metadata": {},
   "source": [
    "#### Step 2: Gradient Sanitization\n",
    "\n",
    "To mimic the Exponential Moving Average (EMA) and gradient sanitization steps:\n",
    "\n",
    "- First, we apply EMA on the gradient over m iterations to smooth the gradient for each client.\n",
    "\n",
    "- Then, we apply Gaussian noise and gradient compression for privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92248791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Exponential Moving Average (EMA)\n",
    "def ema(grad, ema_prev, alpha=0.9):\n",
    "    return (1 - alpha) * ema_prev + alpha * grad\n",
    "\n",
    "# Function for adding noise\n",
    "def add_noise(grad, noise_scale=0.1):\n",
    "    noise = np.random.normal(0, noise_scale, grad.shape)\n",
    "    return grad + noise\n",
    "\n",
    "# Initialize the EMA of gradients\n",
    "ema_gradients = [np.zeros_like(grad1) for _ in range(clients)]\n",
    "\n",
    "# Simulating EMA updates for a few rounds (we'll do just 2 rounds here)\n",
    "for round in range(2):\n",
    "    for i in range(clients):\n",
    "        ema_gradients[i] = ema(gradients[i], ema_gradients[i])\n",
    "        gradients[i] = add_noise(ema_gradients[i])  # Adding noise for sanitization\n",
    "\n",
    "# After sanitization, we get the final gradients\n",
    "sanitized_gradients = gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32fb00f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.39654162,  0.10015513,  0.93914611,  0.88975444],\n",
      "       [ 0.75475388,  0.56983364,  0.52830465,  0.81147938],\n",
      "       [ 0.05758737,  0.13659655, -0.13829024,  0.29499841],\n",
      "       [ 0.34025333,  0.42879517,  0.61986001,  0.33506591]]), array([[ 0.34292976,  0.45251809,  0.08639302,  0.82320196],\n",
      "       [-0.12894248,  0.77494528,  0.7476594 ,  0.28052138],\n",
      "       [-0.11826495,  0.58075678,  0.5609824 ,  0.64794684],\n",
      "       [ 0.68834829,  0.02908005,  0.24010097,  0.1638721 ]]), array([[ 0.97585706,  0.50524518,  0.50756521,  0.09788567],\n",
      "       [-0.01193567,  0.3559341 ,  0.56459811,  0.87420197],\n",
      "       [ 0.89703854,  0.37006446,  0.20084837,  0.57801738],\n",
      "       [ 0.86976661,  0.76250275,  0.74052453,  0.28719755]])]\n"
     ]
    }
   ],
   "source": [
    "print(sanitized_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cceccf0",
   "metadata": {},
   "source": [
    "#### Step 3: Task Relevance Calculation\n",
    "\n",
    "To measure task relevance between clients, we compute cosine similarity between their sanitized gradients. Cosine similarity between two vectors ùëé and ùëè is given by:\n",
    "\n",
    "$$\n",
    "S_{ij} = \\cos(\\theta_{ij}) = \\frac{\\mathbf{a}_i \\cdot \\mathbf{a}_j}{\\lVert \\mathbf{a}_i \\rVert \\,\\lVert \\mathbf{a}_j \\rVert}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65cefda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Relevance Matrix (S):\n",
      "[[1.         0.61306093 0.69216238]\n",
      " [0.61306093 1.         0.63654134]\n",
      " [0.69216238 0.63654134 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a.flatten(), b.flatten()) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Compute the relevance matrix S based on sanitized gradients\n",
    "relevance_matrix = np.zeros((clients, clients))\n",
    "\n",
    "for i in range(clients):\n",
    "    for j in range(clients):\n",
    "        relevance_matrix[i][j] = cosine_similarity(sanitized_gradients[i], sanitized_gradients[j])\n",
    "\n",
    "print(\"Client Relevance Matrix (S):\")\n",
    "print(relevance_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76ed5d",
   "metadata": {},
   "source": [
    "#### Step 4: Global Model Aggregation\n",
    "\n",
    "Now that we have the client relevance matrix, we use it to aggregate the models. We calculate the weighted sum of local models using the relevance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "259c84b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Global Model:\n",
      "[[0.31004383 1.00525158 0.88245523 0.70326347]\n",
      " [0.44765353 0.16355468 0.42864121 0.6941956 ]\n",
      " [0.49128803 0.82773403 0.10677085 0.99249143]\n",
      " [0.70079699 0.38012746 0.45944904 0.36178103]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the local models L (here we use W1, W2, W3 as local models for simplicity)\n",
    "local_models = [W1, W2, W3]\n",
    "\n",
    "# Normalize the relevance matrix to use as weights for aggregation\n",
    "weights = np.exp(relevance_matrix / 1.0)  # Softmax temperature tau = 1.0 for simplicity\n",
    "weights /= np.sum(weights, axis=1, keepdims=True)  # Normalize each row\n",
    "\n",
    "\n",
    "# Weighted sum of local models using relevance weights\n",
    "for i in range(clients):\n",
    "    global_model += weights[i, i] * local_models[i]  # We are aggregating using the relevance of each client to itself\n",
    "\n",
    "\n",
    "print(\"Aggregated Global Model:\")\n",
    "print(global_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c58d7",
   "metadata": {},
   "source": [
    "### Step 5: PQ-LoRA Update\n",
    "\n",
    "For simplicity, assume the PQ-LoRA matrices \\(P\\) and \\(Q\\) are trained locally on each client and aggregated. We update the PQ-LoRA components via:\n",
    "\n",
    "$$\n",
    "h_O = W_p h_I + (1 - \\beta) h_L + \\beta h_G\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $h_I$ is the input hidden state\n",
    "- $W_p$ is the pretrained weight\n",
    "- $h_L$ is the local model output\n",
    "- $h_G$ is the global model output\n",
    "- $\\beta$ is the learnable gating parameter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea42ecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Hidden State (h_I):\n",
      "[[0.97585208 0.51630035 0.32295647 0.79518619]\n",
      " [0.27083225 0.43897142 0.07845638 0.02535074]\n",
      " [0.96264841 0.83598012 0.69597421 0.40895294]\n",
      " [0.17329432 0.15643704 0.2502429  0.54922666]]\n",
      "Local Output (h_L):\n",
      "[[1.43137677 1.31629635 0.8548087  0.95008073]\n",
      " [0.40051697 0.33308839 0.31980512 0.62749895]\n",
      " [0.96626372 0.79011757 0.50672613 1.03706697]\n",
      " [1.07666564 0.70369454 0.45794326 0.84241835]]\n",
      "Global Output (h_G):\n",
      "[[1.54617716 1.44908394 0.96915183 1.01916016]\n",
      " [1.01407045 0.76985295 0.62944526 0.91667897]\n",
      " [0.97837744 0.8615245  0.54627936 1.00041614]\n",
      " [1.29180747 0.96937304 0.66644816 0.95349343]]\n",
      "Gating Parameter (beta):\n",
      "[0.71459592]\n",
      "Output of PQ-LoRA:\n",
      "[[2.94478943 2.72748217 1.79132653 1.94952533]\n",
      " [1.23947675 0.97828696 0.86087782 1.46164475]\n",
      " [1.94118386 1.63126224 1.04171683 2.04794341]\n",
      " [2.30707075 1.59724186 1.06488327 1.76421051]]\n"
     ]
    }
   ],
   "source": [
    "# Assume a simple PQ-LoRA structure where P and Q are trainable low-rank matrices\n",
    "P = np.random.rand(hidden_dim, hidden_dim)\n",
    "Q = np.random.rand(hidden_dim, hidden_dim)\n",
    "\n",
    "# Assume h_I as a random hidden input state\n",
    "h_I = np.random.rand(hidden_dim, hidden_dim)\n",
    "print(\"Input Hidden State (h_I):\")\n",
    "print(h_I)\n",
    "# Compute local output (h_L) and global output (h_G)\n",
    "h_L = np.dot(W1, h_I)\n",
    "h_G = global_model @ h_I\n",
    "print(\"Local Output (h_L):\")\n",
    "print(h_L)\n",
    "print(\"Global Output (h_G):\")\n",
    "print(h_G)\n",
    "# Learnable gating parameter (beta) for blending\n",
    "beta = np.random.rand(1)\n",
    "print(\"Gating Parameter (beta):\")\n",
    "print(beta)\n",
    "\n",
    "# Compute output using PQ-LoRA update\n",
    "h_O = np.dot(W1, h_I) + (1 - beta) * h_L + beta * h_G\n",
    "\n",
    "print(\"Output of PQ-LoRA:\")\n",
    "print(h_O)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e2ba8",
   "metadata": {},
   "source": [
    "#### Step 6: Weight Alignment for Heterogeneous Models\n",
    "\n",
    "The next step is to align the A and B matrices of heterogeneous models across clients. This can be done using L2 loss for ùê¥ and CCA for ùêµ\n",
    "\n",
    "However, in this toy example, we'll skip the alignment as the concept primarily involves adjusting matrices \n",
    "ùê¥ and ùêµ based on shared public data (or pretraining data) and using optimization techniques to minimize the misalignment. You would need a dataset and further implementation to actually perform this alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7562027",
   "metadata": {},
   "source": [
    "#### Step 7: Final Aggregated Model for Clients\n",
    "\n",
    "Finally, after updating the local models with PQ-LoRA and performing weight alignment, the server sends the aggregated model back to the clients for further fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d4e31d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Fine-Tuned Models for Clients:\n",
      "Client 1 Model:\n",
      "[[0.4055445  1.05123946 0.82023947 0.66898483]\n",
      " [0.20078399 0.17234999 0.10094773 0.93559571]\n",
      " [0.65024381 0.79084598 0.03126158 1.069159  ]\n",
      " [0.90252234 0.25035186 0.22776987 0.21958261]]\n",
      "Client 2 Model:\n",
      "[[0.33524663 0.62528159 0.52019054 0.36155549]\n",
      " [0.65661825 0.15584933 0.33500877 0.4357814 ]\n",
      " [0.50519879 0.86794936 0.21035087 0.61348358]\n",
      " [0.66249427 0.08446316 0.65348976 0.20670223]]\n",
      "Client 3 Model:\n",
      "[[0.09605598 1.0494107  1.05387756 0.87872369]\n",
      " [0.34937912 0.11402758 0.72709715 0.50957205]\n",
      " [0.17116704 0.57795031 0.04506561 1.00856954]\n",
      " [0.32885968 0.70053503 0.35765598 0.55624612]]\n"
     ]
    }
   ],
   "source": [
    "# Assume that each client will now fine-tune the global model using their data\n",
    "# For simplicity, we just update the global model slightly\n",
    "for i in range(clients):\n",
    "    local_model = local_models[i] + 0.1 * global_model  # Fine-tune with small learning rate\n",
    "    local_models[i] = local_model  # Update local model\n",
    "\n",
    "print(\"Final Fine-Tuned Models for Clients:\")\n",
    "for i in range(clients):\n",
    "    print(f\"Client {i+1} Model:\")\n",
    "    print(local_models[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
